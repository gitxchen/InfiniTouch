{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Capacitive Images with OptiTrack Ground Truth\n",
    "This script performs a simple merge of all data sources: front, back, side, and the OptiTrack timestamps. The result are pickle files with each row representing the capacitive image and the marker positions at one point in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.interpolate as interp\n",
    "\n",
    "from skimage import measure\n",
    "from skimage.measure import find_contours, approximate_polygon, \\\n",
    "    subdivide_polygon, EllipseModel, LineModelND\n",
    "    \n",
    "from datetime import datetime\n",
    "\n",
    "%run py/labeling_names.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH_TIMESTAMPS = \"./data/timestamps/\"\n",
    "DATA_PATH_FRONT = \"./data/front/\"\n",
    "DATA_PATH_BACK = \"./data/back/\"\n",
    "DATA_PATH_SIDE = \"./data/side/\"\n",
    "\n",
    "PREFIX_TIMESTAMPS = \"timestamps_\"\n",
    "PREFIX_FRONT = \"front_\"\n",
    "PREFIX_BACK  = \"back_\"\n",
    "PREFIX_SIDE  = \"side_\"\n",
    "\n",
    "df = \"\"\n",
    "dfTimestamp = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_matrix(s):\n",
    "    matrix = s.replace(\"\\n\", \"\")\n",
    "    matrix = matrix.split(\",\")\n",
    "\n",
    "    if (len(matrix) != 408):\n",
    "        return -1, -1\n",
    "    \n",
    "    matrix = matrix[:407]\n",
    "    \n",
    "    # determine timestamp\n",
    "    timestamp = str(matrix[0]) + str(matrix[1][:3])\n",
    "    timestamp = int(timestamp)\n",
    "    \n",
    "    matrix = np.array(matrix[2:]).reshape(27, 15)\n",
    "    \n",
    "    matrix[matrix==''] = '0'\n",
    "    matrix[matrix=='-'] = '0'\n",
    "    \n",
    "    try:\n",
    "        matrix = matrix.astype(int)\n",
    "    except ValueError:\n",
    "        return -1, -1\n",
    "    \n",
    "    return matrix, timestamp\n",
    "\n",
    "def getMatrixLeft(x):\n",
    "    if (type(x) is int):\n",
    "        return -1\n",
    "    else: \n",
    "        return x[:16]\n",
    "\n",
    "def getMatrixBottom(x):\n",
    "    if (type(x) is int):\n",
    "        return -1\n",
    "    else: \n",
    "        return x[16:21]\n",
    "\n",
    "def getMatrixRight(x):\n",
    "    if (type(x) is int):\n",
    "        return -1\n",
    "    else: \n",
    "        return x[21:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log(s):\n",
    "    with open(\"status_PY02_preprocessing.txt\", \"a\") as myfile:\n",
    "        myfile.write(\"[\" + str(datetime.now()) + \"] \" + s + \"\\n\")\n",
    "    print(\"[\" + str(datetime.now()) + \"] \" + s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-07-13 11:31:51.517632] P16 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.519097] P18 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.519356] P17 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.519608] P6 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.519861] P19 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.520109] P13 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.520355] P4 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.520599] P10 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.520886] P21 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.521144] P8 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.521423] P3 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.521680] P2 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.522014] P22 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.522329] P12 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.522598] P5 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.522859] P9 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.523120] P20 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.523384] P11 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.523639] P15 is already available. Skipped.\n",
      "[2018-07-13 11:31:51.523932] Reading matrices of P14\n",
      "[2018-07-13 11:33:24.899881] Reading OptiTrack data of P14\n",
      "[2018-07-13 11:33:32.015897] Splitting side into left, bottom and right for P14\n",
      "[2018-07-13 11:33:33.777857] Creating merged matrix for P14\n",
      "[2018-07-13 11:47:59.882987] Saving P14\n"
     ]
    }
   ],
   "source": [
    "df = \"\"\n",
    "dfTimestamp = \"\"\n",
    "    \n",
    "for optitrack_filename in os.listdir(\"./data/transformed_optitrack/\"):\n",
    "    if optitrack_filename.endswith(\".pkl\"):\n",
    "        current_participant = optitrack_filename.split(\"_\")[1].split(\".\")[0]\n",
    "        \n",
    "        if os.path.isfile(\"./data/pickles/raw_data_\" + str(current_participant) + \".pkl\") :\n",
    "            log(current_participant + \" is already available. Skipped.\")\n",
    "            continue\n",
    "        \n",
    "        log(\"Reading matrices of \" + current_participant)\n",
    "        df_temp = pd.read_csv(DATA_PATH_FRONT + PREFIX_FRONT + current_participant + \".txt\", header=None, sep=\";\", names=[\"Participant\", \"Source\", \"Capstr\"])\n",
    "        df_temp['Timestamp'] = df_temp.Capstr.apply(lambda x : transform_to_matrix(x)[1])\n",
    "        df_temp['Matrix'] = df_temp.Capstr.apply(lambda x : transform_to_matrix(x)[0])\n",
    "        df_temp = df_temp.drop(['Capstr'], axis=1)\n",
    "        df_temp = df_temp[~(df_temp.Timestamp == -1)]\n",
    "        df = df_temp\n",
    "        \n",
    "        df_temp = pd.read_csv(DATA_PATH_BACK + PREFIX_BACK + current_participant + \".txt\", header=None, sep=\";\", names=[\"Participant\", \"Source\", \"Capstr\"])\n",
    "        df_temp['Timestamp'] = df_temp.Capstr.apply(lambda x : transform_to_matrix(x)[1])\n",
    "        df_temp['Matrix'] = df_temp.Capstr.apply(lambda x : transform_to_matrix(x)[0])\n",
    "        df_temp = df_temp.drop(['Capstr'], axis=1)\n",
    "        df_temp = df_temp[~(df_temp.Timestamp == -1)]\n",
    "        df = df.append(df_temp)\n",
    "        \n",
    "        df_temp = pd.read_csv(DATA_PATH_SIDE + PREFIX_SIDE + current_participant + \".txt\", header=None, sep=\";\", names=[\"Participant\", \"Source\", \"Timestamp\", \"Matrix\"])\n",
    "        df_temp['Matrix'] = df_temp.Matrix.apply(lambda x : np.fromstring(x, sep=\",\"))\n",
    "        df_temp = df_temp[~(df_temp.Timestamp == -1)]\n",
    "        df = df.append(df_temp)\n",
    "        \n",
    "        df_temp = pd.read_csv(DATA_PATH_TIMESTAMPS + PREFIX_TIMESTAMPS + current_participant + \".txt\", header=None, sep=\",\", names=[\"Front\", \"Back\", \"Side\", \"OptiTrack\"])\n",
    "        dfTimestamp = df_temp\n",
    "        \n",
    "        log(\"Reading OptiTrack data of \" + current_participant)\n",
    "        dfOpti = pd.read_pickle(\"./data/transformed_optitrack/\" + optitrack_filename)\n",
    "        \n",
    "        # replace participant IDs\n",
    "        df.Participant = df.Participant.replace(PARTICIPANT_ID_REPLACES)\n",
    "\n",
    "        df_back = df[df.Source == \"BACK\"].copy(deep=True).sort_values(by=['Timestamp']).rename(columns={'Timestamp': 'Back_Timestamp'})\n",
    "        df_front = df[df.Source == \"FRONT\"].copy(deep=True).sort_values(by=['Timestamp']).rename(columns={'Timestamp': 'Front_Timestamp'})\n",
    "        df_side = df[df.Source == \"SIDE\"].copy(deep=True).sort_values(by=['Timestamp']).rename(columns={'Timestamp': 'Side_Timestamp'})\n",
    "\n",
    "        dfOpti.Time = dfOpti.Time.astype(np.int64)\n",
    "        dfOpti = dfOpti.rename(columns={'Time': 'OptiTrack_Timestamp'})\n",
    "        dfOpti = dfOpti.sort_values(by=['OptiTrack_Timestamp'])\n",
    "\n",
    "        dfTimestamp = dfTimestamp.rename(columns={\"Front\": \"Front_Timestamp\", \"Back\": \"Back_Timestamp\", \"Side\": \"Side_Timestamp\", \"OptiTrack\": \"OptiTrack_Timestamp\"})\n",
    "\n",
    "        ##########\n",
    "        ## FRONT TIMESTAMP\n",
    "        ##########\n",
    "        dfSynced = pd.merge_asof(dfTimestamp.sort_values(by=['Front_Timestamp']), df_front, on='Front_Timestamp', direction='nearest')\n",
    "\n",
    "        # Leave some of the columns for sanity checks\n",
    "        dfSynced = dfSynced.drop(['Participant'], axis=1)\n",
    "        dfSynced = dfSynced.drop(['Source'], axis=1)\n",
    "        dfSynced = dfSynced.rename(columns={'Matrix': 'MatrixFront'})\n",
    "\n",
    "        ##########\n",
    "        ## BACK TIMESTAMP\n",
    "        ##########\n",
    "        dfSynced = dfSynced.sort_values(by=['Back_Timestamp'])\n",
    "        dfSynced = pd.merge_asof(dfSynced, df_back, on='Back_Timestamp', direction='nearest')\n",
    "\n",
    "        # Leave some of the columns for sanity checks\n",
    "        dfSynced = dfSynced.drop(['Participant'], axis=1)\n",
    "        dfSynced = dfSynced.drop(['Source'], axis=1)\n",
    "        dfSynced = dfSynced.rename(columns={'Matrix': 'MatrixBack'})\n",
    "\n",
    "        ##########\n",
    "        ## SIDE TIMESTAMP\n",
    "        ##########\n",
    "        dfSynced = dfSynced.sort_values(by=['Side_Timestamp'])\n",
    "        dfSynced = pd.merge_asof(dfSynced, df_side, on='Side_Timestamp', direction='nearest')\n",
    "\n",
    "        # Leave some of the columns for sanity checks\n",
    "        dfSynced = dfSynced.drop(['Participant'], axis=1)\n",
    "        dfSynced = dfSynced.drop(['Source'], axis=1)\n",
    "        dfSynced = dfSynced.rename(columns={'Matrix': 'MatrixSide'})\n",
    "\n",
    "        ##########\n",
    "        ## OPTITRACK DATA\n",
    "        ##########\n",
    "        dfSynced = dfSynced.sort_values(by=['OptiTrack_Timestamp'])\n",
    "        dfSynced = pd.merge_asof(dfOpti, dfSynced, on='OptiTrack_Timestamp', direction='nearest')\n",
    "\n",
    "        log(\"Splitting side into left, bottom and right for \" + current_participant)\n",
    "        dfSynced['MatrixLeft'] = dfSynced.MatrixSide.apply(lambda x : getMatrixLeft(x))\n",
    "        dfSynced['MatrixBottom'] = dfSynced.MatrixSide.apply(lambda x :getMatrixBottom(x))\n",
    "        dfSynced['MatrixRight'] = dfSynced.MatrixSide.apply(lambda x : getMatrixRight(x))\n",
    "\n",
    "        log(\"Creating merged matrix for \" + current_participant)\n",
    "        merged = []\n",
    "        for index, d in dfSynced.iterrows():\n",
    "            if (not type(d.MatrixFront) is int and \n",
    "                not type(d.MatrixBack) is int and\n",
    "                not type(d.MatrixLeft) is int and\n",
    "                not type(d.MatrixRight) is int and\n",
    "                not type(d.MatrixBottom) is int):\n",
    "\n",
    "                rear_img = np.fliplr(d.MatrixBack)\n",
    "                front_img = d.MatrixFront\n",
    "                left_img = d.MatrixLeft.T\n",
    "                right_img = d.MatrixRight\n",
    "                right_img = np.insert(right_img, 0, 0, axis=0)\n",
    "                right_img = right_img.T\n",
    "                bot_img = d.MatrixBottom\n",
    "                front_back_merged = rear_img\n",
    "                new_size = 27\n",
    "\n",
    "                # Interpolate left, right and bottom edges\n",
    "                left_interp = interp.interp1d(np.arange(left_img.size), left_img)\n",
    "                left_stretch = left_interp(np.linspace(0, left_img.size - 1, new_size))\n",
    "                left_stretch = left_stretch.reshape(27, 1)\n",
    "\n",
    "                right_interp = interp.interp1d(np.arange(right_img.size), right_img)\n",
    "                right_stretch = right_interp(np.linspace(0, right_img.size - 1, new_size))\n",
    "                right_stretch = right_stretch.reshape(27, 1)\n",
    "\n",
    "                bot_interp = interp.interp1d(np.arange(bot_img.size), bot_img)\n",
    "                bot_stretch = bot_interp(np.linspace(0, bot_img.size - 1, 15))\n",
    "                bot_stretch = np.insert(bot_stretch, 0, 0) # a zero at the left corner\n",
    "                bot_stretch = np.insert(bot_stretch, bot_stretch.size, 0) # zero at the right corner\n",
    "\n",
    "                # merge back image with interpolated left, right and bottom edges\n",
    "                merged_img = np.hstack((left_stretch, front_back_merged))\n",
    "                merged_img = np.hstack((merged_img, right_stretch))\n",
    "                merged_img = np.vstack((merged_img.reshape(-1, 17), bot_stretch.T))\n",
    "                \n",
    "                # also merge with front image on the left side\n",
    "                front_extended = np.vstack((front_img, (np.zeros(15)))) \n",
    "                merged_img = np.hstack((merged_img, front_extended))\n",
    "                \n",
    "                merged.append(merged_img)\n",
    "            else:\n",
    "                merged.append(-1)\n",
    "\n",
    "        dfSynced['MatrixMerged'] = merged\n",
    "\n",
    "        log(\"Saving \" + current_participant)\n",
    "        pd.to_pickle(dfSynced, \"./data/pickles/\" + \"raw_data_\" + current_participant + \".pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
